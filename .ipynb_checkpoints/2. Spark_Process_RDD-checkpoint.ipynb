{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the data files information from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_url = 'bittigermusicplayerdata.s3-us-west-2.amazonaws.com/'\n",
    "s3_bucket = 'bittigermusicplayerdata'\n",
    "\n",
    "r = requests.get('http://' + s3_url)\n",
    "\n",
    "soup = BeautifulSoup(r.content, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(soup):\n",
    "    search_file = []\n",
    "    play_file = []\n",
    "    download_file = []\n",
    "    for file in soup.findAll('Contents'):\n",
    "        tags = file.findAll('Key')\n",
    "        if len(tags) == 1:\n",
    "            tags_text = tags[0].text\n",
    "            \n",
    "            # add to search file\n",
    "            if 'search' in tags_text:\n",
    "                search_file.append(tags_text)\n",
    "            # add to play file\n",
    "            if 'play' in tags_text:\n",
    "                play_file.append(tags_text)            \n",
    "            # add to download file\n",
    "            if 'down' in tags_text:\n",
    "                download_file.append(tags_text)            \n",
    "        else:\n",
    "            print('Error when loading file to list')\n",
    "    return search_file, play_file, download_file\n",
    "\n",
    "def sub_list(s_list, p_list, d_list, dates):\n",
    "    new_s_list = []\n",
    "    new_p_list = []\n",
    "    new_d_list = []\n",
    "    for date in dates:\n",
    "        in_s_list = list(filter(lambda x: date in x, s_list))\n",
    "        in_p_list = list(filter(lambda x: date in x, p_list))\n",
    "        in_d_list = list(filter(lambda x: date in x, d_list))\n",
    "        if len(in_s_list) > 0 and len(in_p_list) > 0 and len(in_d_list) > 0:\n",
    "            new_s_list.append(in_s_list)\n",
    "            new_p_list.append(in_p_list)\n",
    "            new_d_list.append(in_d_list)\n",
    "            \n",
    "    return new_s_list, new_p_list, new_d_list\n",
    "\n",
    "def file_parser(soup, dates):\n",
    "    s_list, p_list, d_list = get_file_list(soup)\n",
    "    if dates:\n",
    "        s_list, p_list, d_list = sub_list(s_list, p_list, d_list, dates)\n",
    "        return [item for group in s_list for item in group], \\\n",
    "        [item for group in p_list for item in group], \\\n",
    "        [item for group in d_list for item in group]\n",
    "    return s_list, p_list, d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_file, play_file, download_file = file_parser(soup, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAccessKey = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "mySecretKey = os.getenv('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', myAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', mySecretKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"s3a://bittigermusicplayerdata/1_3_search.log.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.544083\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "textFile.count()\n",
    "end = datetime.datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing search file logic\n",
    "1. define parsing logic for single search log file\n",
    "2. define pipeline for multiple search log files\n",
    "3. define output function to save parsed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_unpersist():\n",
    "    for (id_, rdd) in sc._jsc.getPersistentRDDs().items():\n",
    "        rdd.unpersist()\n",
    "    print(sc._jsc.getPersistentRDDs().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_log(bucket, key, propertion=False):\n",
    "    # load search file\n",
    "    rdd_feed = sc.textFile(\"s3a://\"+ bucket + '/' + key)\n",
    "    if propertion:\n",
    "        rdd_feed = rdd_feed.sample(False, propertion, seed=66)\n",
    "    \n",
    "    # get separate rows by \\t\n",
    "    def strip_elem(ls):\n",
    "        return list(map(lambda x: x.rstrip().lstrip(), ls))\n",
    "\n",
    "    rdd = rdd_feed.map(lambda x: x.split('\\t'))\n",
    "    rdd = rdd.map(strip_elem)\n",
    "\n",
    "    # get each row length\n",
    "    rdd = rdd.map(lambda x: (x, len(x)))\n",
    "    # print(rdd.count())\n",
    "\n",
    "    # filter out row length < 4\n",
    "    rdd = rdd.filter(lambda x: x[1] == 4)\n",
    "    # print(rdd.count())\n",
    "\n",
    "    # filter out id not match digit\n",
    "    rdd = rdd.filter(lambda x: re.match('^\\d+$', x[0][0]))\n",
    "    # print(rdd.count())\n",
    "\n",
    "    rdd = rdd.map(lambda x: x[0][:3])\n",
    "\n",
    "    # split date column to date and time\n",
    "    rdd = rdd.map(lambda x: [*x[:2], *x[2].split(' ')])\n",
    "\n",
    "    # reduce by id and add them up\n",
    "    rdd = rdd.map(lambda x: (','.join([x[0], x[2]]), 1)).reduceByKey(lambda x, y: x + y)\n",
    "    rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "    rdd = rdd.map(lambda x: [*x[0].split(','), x[1]])\n",
    "    # print(rdd.count())\n",
    "\n",
    "    return rdd\n",
    "\n",
    "def cnt_rows(bucket, key):\n",
    "    rdd_feed = sc.textFile(\"s3a://\"+ bucket + '/' + key)\n",
    "    cnt = rdd_feed.count()\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_search_rdds(rdd_union):\n",
    "    rdd_union = rdd_union.map(lambda x: (','.join([x[0], x[1]]), x[2])).reduceByKey(lambda x, y: x + y)\n",
    "    rdd_union = rdd_union.map(lambda x: [*x[0].split(','), x[1]])\n",
    "    return rdd_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_parse_pipeline(bucket, search_files):\n",
    "    rdd_list = list(map(lambda x: parse_search_log(bucket, x), search_files))\n",
    "    rdd_union = reduce(lambda x, y: x.union(y), rdd_list)\n",
    "    return rdd_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(out, fields, files):\n",
    "    with open(out, 'w') as f:\n",
    "        csvout = csv.writer(f, delimiter=',')\n",
    "        csvout.writerow(fields)\n",
    "        for line in files:\n",
    "            csvout.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse log files & write condensed files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of rows: 9795112\n",
      "Spend 167.83 seconds...\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "# count number of rows\n",
    "row_num = sum(list(map(lambda x: cnt_rows(s3_bucket, x), search_file)))\n",
    "print('Total Number of rows: %s'%(row_num))\n",
    "\n",
    "# get etl files\n",
    "search_date = search_parse_pipeline(s3_bucket, search_file).collect()\n",
    "end = datetime.datetime.now()\n",
    "print('Spend %.2f seconds...'%((end - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('./dataset/search_log_freq.csv', ['uuid', 'date', 'freq'], search_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[2]')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1537631863999'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.master', 'local[2]'),\n",
       " ('spark.driver.port', '51305'),\n",
       " ('spark.driver.host', 'zihaos-mbp.fios-router.home')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='table', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/Users/aiweiwei/Desktop/projects/music_box_spark/spark-warehouse')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('dataset/1_1_search.csv', header=True) # if small enough than you can use cache to persist/cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------------+\n",
      "|     uuid|device|               date|\n",
      "+---------+------+-------------------+\n",
      "|154436633|    ip|2017-03-01 00:00:24|\n",
      "|154407262|    ar|2017-03-01 00:00:53|\n",
      "|154407854|    ip|2017-03-01 00:00:54|\n",
      "|154407252|    ar|2017-03-01 00:00:55|\n",
      "|154407327|    ar|2017-03-01 00:00:55|\n",
      "|154407255|    ip|2017-03-01 00:00:56|\n",
      "|154407261|    ip|2017-03-01 00:00:59|\n",
      "|154407267|    ar|2017-03-01 00:00:59|\n",
      "|154407546|    ip|2017-03-01 00:01:00|\n",
      "|154407254|    ar|2017-03-01 00:01:02|\n",
      "|154407198|    ar|2017-03-01 00:01:06|\n",
      "|154407244|    ar|2017-03-01 00:01:06|\n",
      "|154407261|    ip|2017-03-01 00:01:07|\n",
      "|154407362|    ar|2017-03-01 00:01:08|\n",
      "|154407377|    ar|2017-03-01 00:01:08|\n",
      "|154407348|    ar|2017-03-01 00:01:09|\n",
      "|154407303|    ar|2017-03-01 00:01:14|\n",
      "|154407406|    ar|2017-03-01 00:01:14|\n",
      "|154407358|    ar|2017-03-01 00:01:15|\n",
      "|154407327|    ar|2017-03-01 00:01:17|\n",
      "+---------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use persist in regular basis; cache is synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only remove from cache\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from cache and from tablelist\n",
    "spark.catalog.dropTempView(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid: string, device: string, date: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------------+\n",
      "|     uuid|device|               date|\n",
      "+---------+------+-------------------+\n",
      "|154436633|    ip|2017-03-01 00:00:24|\n",
      "|154407262|    ar|2017-03-01 00:00:53|\n",
      "|154407854|    ip|2017-03-01 00:00:54|\n",
      "|154407252|    ar|2017-03-01 00:00:55|\n",
      "|154407327|    ar|2017-03-01 00:00:55|\n",
      "|154407255|    ip|2017-03-01 00:00:56|\n",
      "|154407261|    ip|2017-03-01 00:00:59|\n",
      "|154407267|    ar|2017-03-01 00:00:59|\n",
      "|154407546|    ip|2017-03-01 00:01:00|\n",
      "|154407254|    ar|2017-03-01 00:01:02|\n",
      "+---------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM table\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
